---
title: "Final Project Placeholder"
subtitle: "Maybe you have a subtitle"
author: "David Sneddon"
institute: "Old Dominion University"
format: 
  html:
    theme: lux # Check here for more themes: https://quarto.org/docs/output-formats/html-themes.html
    code-tools: true
    code-fold: true
    code-summary: "Code"
    code-copy: hover
    link-external-newwindow: true
    tbl-cap-location: top
    fig-cap-location: bottom

self-contained: true
editor: source
---

```{r setup, include=FALSE}
# DO NOT EDIT THIS

knitr::opts_chunk$set(fig.align = 'center')
knitr::opts_chunk$set(out.width = '90%')
knitr::opts_chunk$set(results = 'hold')
knitr::opts_chunk$set(fig.show = 'hold')
knitr::opts_chunk$set(error = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
par(mar = c(4.1, 4.1, 1.1, 4.1))

hooks = knitr::knit_hooks$get()
hook_foldable = function(type) {
  force(type)
  function(x, options) {
    res = hooks[[type]](x, options)
    
    if (is.na(as.logical(options[[paste0("fold.", type)]]))) {
      
      res
    } else if (isFALSE(as.logical(options[[paste0("fold.", type)]]))){
      
      # return(res)
      
      paste0(
      "<details open><summary>", gsub("^p", "P", gsub("^o", "O", type)), "</summary>\n\n",
      res,
      "\n\n</details>"
      )
    } else {
      
      paste0(
      "<details><summary>", gsub("^p", "P", gsub("^o", "O", type)), "</summary>\n\n",
      res,
      "\n\n</details>"
      )
    }
  }
}

knitr::knit_hooks$set(
  output = hook_foldable("output"),
  plot = hook_foldable("plot")
)
knitr::opts_chunk$set(fold.output=TRUE)
knitr::opts_chunk$set(fold.plot=TRUE)

Q <- 0
```

*For some guidance on what your project should look like, check out this [sample paper by Dr. Tomas Dvorak](https://bpb-us-w2.wpmucdn.com/muse.union.edu/dist/d/377/files/2016/05/sample_paper.pdf).*

## Introduction

*Motivate and introduce your topic. Convince the reader that this topic/question is important and that they should care. What data do you use? How do you model your outcome variable(s)? What do you find? What is learned from your analysis?*

For a "how to" on writing introductions, see [Keith Head's Introduction Formula](https://blogs.ubc.ca/khead/research/research-advice/formula).

## Literature Review

*What work has already been done on this topic by others? What are their conclusions? How is your work different? If your topic is time-sensitive or in the news, you may discuss current events here as well.*

## Data

*Where does your data come from? Why is this data good for answering your question? Be sure to create, and discuss, a summary statistics table and some plots.*

```{r}
#LIBRARIES
library(readr)
library(geosphere)
library(utf8)
library(statcanR)
library(bea.R)
library(censusapi)
library("ggplot2")
library("sf")
library("rnaturalearth")
library("rnaturalearthdata")
library("canadianmaps")
library(plyr)
library(stringr)
library(usmap)
library(spData) 
library("tools")
library("modelsummary")
options("modelsummary_factory_default" = "kableExtra")
options(scipen = 999)

#FUNCTION FOR CREATING MAPS
gravitymap <- function (x){
  theme_set(theme_bw())
  sf_use_s2(FALSE)
  
  worldz <- ne_countries(scale = "large", returnclass = "sf")
  class(worldz)
  (sites <- data.frame(longitude = c(-172.54, -47.74), latitude = c(23.81,
                                                                    90)))
  
  
  (sites <- st_as_sf(geodata, coords = c("lon", "lat"),
                     crs=4369, agr = "constant"))
  
  sites <- st_transform(sites, st_crs("ESRI:102010"))
  states <- st_transform(us_states, st_crs("ESRI:102010"))
  head(states)
  provs <- st_sf(PROV[,c(5, 10, 11, 12)])
  ak <- st_transform(alaska, st_crs("ESRI:102010"))
  hi <- st_transform(hawaii, st_crs("ESRI:102010"))
  akhi <- rbind(ak,hi)
  akhi <- cbind(akhi, st_coordinates(st_centroid(akhi)))
  akhi <- st_sf(akhi[c(2,7,8,9)])
  provs <- st_transform(provs, st_crs("ESRI:102010"))
  
  
  states <- cbind(states, st_coordinates(st_centroid(states)))
  states <- states[c(2, 7, 8, 9)]
  colnames(provs) <- colnames(akhi) <- colnames(states)
  
  head(states)

  
  head(states)
  stpr <- st_sf(rbind(as.data.frame(states),
                      as.data.frame(provs),
                      as.data.frame(akhi)))
  
  tf <- data.frame(us_ca_split[x])
  prov_abr2 <- prov_abr
  colnames(prov_abr2) <- colnames(st_abbr)
  tabrs <- rbind(st_abbr, prov_abr2)
  colnames(tabrs)[1] <- "NAME"
  colnames(tf) <- colnames(us_ca)
  tf$value <- asinh(tf$value)
  tf <- tf[c(1, 7)]
  colnames(tf)[1] <- "State"
  tstpr <- merge(stpr, tabrs, by = "NAME")
  tstpr <- merge(tstpr, tf, by = "State")
  
  ggplot(data = worldz) +
    geom_sf() +
    ggtitle(paste("Import Gravity Map for", prov_abr[prov_abr$from==x,])) +
    geom_sf(data = tstpr, 
            aes(fill =value)) +
    scale_fill_gradient(low="red", high= "green") +
    coord_sf(xlim = c(-6500000, 3800000), 
             ylim = c(4700000, -1700000), 
             expand = FALSE, 
             crs = st_crs("ESRI:102010"))

}


#DATA IMPORTS
url_df <- read.csv("url_df.csv")
usfips <- read.csv("https://www2.census.gov/geo/docs/reference/state.txt",
                   sep = "|")
usfips <- usfips[c(1, 2)]
colnames(usfips) <- c("STATE", "state")
##POPULATION
###US
st_pop <- getCensus(
  name = "dec/dhc",
  vars = "P1_001N",
  region = "state:*",
  vintage = 2020
)
st_pop$state <- as.numeric(st_pop$state)
colnames(st_pop) <- c("STATE", "pop")
st_pop <- merge(st_pop, usfips, by="STATE")
st_pop <- st_pop[,c(3, 2)]
###CANADA

##GEOGRAPHIC DATA
###Centers of Population
st_lat_long <- read_csv("st_pop_center.csv") #From Census.gov - 2020
canprov <- read_csv("canprovcenter.csv") #Calculated with ChatGPT - 2021
st_abbr <- read_csv("st_abbr.csv")


#DISTANCE DATA
geodata <- as.data.frame(rbind(as.matrix(st_lat_long),
                               as.matrix(canprov)))
geoframe <- data.frame(matrix(0, nrow(geodata)^2, 3))
colnames(geoframe) <- c("from", "to", "km")
c <- 1
for (i in geodata[,1]){
  for (w in geodata[,1]){
    
    lat_a <- as.numeric(geodata[geodata$state==i,2])
    lat_b <- as.numeric(geodata[geodata$state==w,2])
    lon_a <- as.numeric(geodata[geodata$state==i,3])
    lon_b <- as.numeric(geodata[geodata$state==w,3])
    geoframe[c,1] <- i
    geoframe[c,2] <- w
    geoframe[c,3] <- as.numeric(distHaversine(c(lon_a, lat_a),
                                              c(lon_b, lat_b)))/1000
    c <- c+1
  }
}
geoframe["fttag"] <- paste0(geoframe$from,geoframe$to)
geoframe <- geoframe[,c(3, 4)]




##CANADIAN DATA
can_exports <- read.csv(url_df$candata)
can_exports <- can_exports[,c(2, 4, 5, 12)]
#DATAFRAME FOR US STATE AND PROVINCIAL ABBREVIATIONS FOR UNIFORMITY
#NEEDED FOR MERGING
prov_abr <- data.frame(GEO = unique(can_exports$GEO))
prov_abr["from"] <- c("NL", "PE", "NS", "NB", "QC", "ON", "MB", "SK", "AB", 
                      "BC", "YT", "NT", "NU")
provto_abr <- prov_abr
provto_abr$GEO <- paste("To", prov_abr$GEO)
colnames(provto_abr) <- c("To_GEO","to")
colnames(can_exports)[2] <- "To_GEO"
#FORMAT can_exports  FOR MERGING
can_exports <- merge(can_exports, prov_abr, by="GEO")
can_exports <- merge(can_exports, provto_abr, by="To_GEO")
rm(provto_abr)
can_exports["value"] <- can_exports$VALUE*1000
can_exports <- can_exports[,-(1:4)]
can_exports["fttag"] <- paste0(can_exports$to,can_exports$from)
can_exports["intra"] <- -1
for (i in 1:nrow(can_exports)){
  if(can_exports$from[i]==can_exports$to[i]){
    can_exports$intra[i] <- 1
  }else
    can_exports$intra[i] <- 0
}
can_exports <- can_exports[can_exports$intra == 0,]
can_exports$intra <- NULL
tmp <- can_exports
can_exports <- tmp[,c(2,1,3,4)]
rm(tmp)
can_exports["domestic"] <- 0


###IMPORT AGGREGATE AND CONDENSE CAN_IMPORTS DATAFRAME
can_imports <- read.csv("ODPFN022_201912N.csv")
can_imports <- can_imports[can_imports$Country.Pays=="US",]
colnames(can_imports) <- c("YearMonth", 
                           "HS2", 
                           "Country", 
                           "Province", 
                           "State", 
                           "Value")
can_imports <- aggregate(can_imports$Value
                         ~ can_imports$Country
                         + can_imports$Province
                         + can_imports$State,
                         FUN = sum)
for (i in 1:nrow(can_imports)){
  for (u in 1:3){
    can_imports[i,u] <- as_utf8(can_imports[i,u])
  }
}
can_imports <- can_imports[,c(2, 3, 4)]
colnames(can_imports) <- c("to", "from", "value")
can_imports["fttag"] <- paste0(can_imports$to,can_imports$from)
can_imports["domestic"] <- 1

###CANADIAN GDP
can_gdp <- read.csv(url_df$can_gdp)
can_gdp <- can_gdp[,c("GEO","VALUE")]
can_gdp <- merge(can_gdp, prov_abr, by="GEO")
cagdp_f <- data.frame(from=can_gdp$from,
                      from_gdp=can_gdp$VALUE)
cagdp_t <- data.frame(to=can_gdp$from,
                      to_gdp=can_gdp$VALUE)
##MERGE CANADIAN IMPORTS AND EXPORTS

##US DATA
bkey <- "EB73777D-FF2C-43F1-A94A-E1BFFBA2744D"

userSpecList <- list('UserID' = bkey,
                     'Method' = 'GetData',
                     'datasetname' = 'Regional',
                     'GeoFips' = 'STATE',
                     'LineCode' = '3',
                     'TableName' = 'SAGDP1',
                     'Year' = '2019')
us_gdp <- beaGet(userSpecList)
us_gdp <- us_gdp[-1,]
us_gdp <- us_gdp[-(52:59),]
us_gdp <- us_gdp[,c(3, 6)]
exch_usca <- 1.3269 #USD TO CAD 2019 https://www.bankofcanada.ca/rates/exchange/annual-average-exchange-rates/
us1219 <- 1.07 #$1 USD IN 2012 IN 2019 DOLLARS 
us_gdp <- merge(us_gdp, st_abbr, by="GeoName")
usgdp_f <- data.frame(from=us_gdp$State,
                      from_gdp=as.numeric(us_gdp$`DataValue_2019`)*us1219*exch_usca)
usgdp_t <- data.frame(to=us_gdp$State,
                      to_gdp=as.numeric(us_gdp$`DataValue_2019`)*us1219*exch_usca)
rm(us_gdp, exch_usca, us1219)
#MERGE BOTH COUNTRIES GDPs
gdp_f <- rbind(cagdp_f,usgdp_f)
rm(cagdp_f, usgdp_f)
gdp_t <- rbind(cagdp_t,usgdp_t)
rm(cagdp_t, usgdp_t)
#PULL TOGETHER can_exports, can_imports, geoframe, and GDPs
us_ca <- rbind(can_imports, can_exports)
us_ca <- merge(us_ca, geoframe, by = "fttag")
us_ca <- merge(us_ca, gdp_f, by = "from")
us_ca <- merge(us_ca, gdp_t, by = "to")
us_ca <- us_ca[,c(2,1,7,8,5,6,4)]
#SPLIT TO EACH PROVINCE
us_ca_split <- split(us_ca, list(us_ca$to))
#REGRESSION

reg2 <- lm(asinh(value) ~ asinh(from_gdp) +
             asinh(to_gdp) + asinh(km) +
             domestic, data=us_ca)
summary(reg2)

# for(i in unique(prov_abr$from)){
#   ggsave(paste0(i,".png"),gravitymap(i),
#          device = "png",
#          path="maps",
#          create.dir = TRUE)
# }

plot ((us_ca$km), asinh(us_ca$value),
      pch = 20,
      cex = 0.5,
      main = "Distance vs Total Goods Imports",
      xlab = "Distance (km)",
      ylab = "arsinh(Goods Imports)",
      col = alpha("red",0.5),
      las = 1
      )

title <- "US Canada Trade Data - Summary Statistics"
frmla <- (`Distance (km)` = km) + 
  (`Imports` = value)  ~ 
    (`N` = length) + 
    Mean + 
    (`St. Dev.` = sd) + 
    (`Min` = min) + 
    (`Max` = max)
#Output a table
datasummary(frmla, data = us_ca, title = title, fmt = fmt_significant(2))
```

## Empirical Model

*Write down the model(s) you estimate. Explain your modeling choices (e.g. log(), etc.). List out your hypotheses and the rationales behind them.*

$$\text{Outcome}_{it} = \alpha + \delta D_{it} + \epsilon_{it}$$

For inline equations, only use a single dollar sign like: $\overline{Y} = \widehat{m}\overline{X} + \widehat{b}$.

## Results

*Discuss the results of your analysis. Interpret the coefficients.*

```{r}
2 + 2
```

## Conclusion

*Remind the reader of your topic, why it is important, and what you find. Be sure to include a discussion of the implications of your findings.*

Marc Bellemare has a helpful [Conclusion Formula](https://marcfbellemare.com/wordpress/12060) in response to Dr. Head's.
